# H100 Stable Training - AUDIT FIXED
# Changes from audit: smaller model, lower LR, early stopping

# Model Architecture - REDUCED to prevent overfitting (68 samples)
model:
  n_layer: 4            # Reduced from 6 (less capacity = less overfit)
  n_embd: 128           # Reduced from 256 (4x fewer params)
  n_head: 4
  mlp_multiplier: 64    # Reduced from 128 (2x fewer params)
  vocab_size: 256
  dropout: 0.3          # Increased from 0.2 (more regularization)

# Retrieval - reduced for speed
retrieval:
  top_k: 15             # Reduced from 30 (faster, still sufficient)
  similarity_type: cosine

# Aggregation
aggregation:
  strategy: max
  top_k: 3

# Data Processing - reduced sequence lengths
data:
  chunk_size: 768       # Reduced from 1024
  chunk_overlap: 192
  max_backstory_tokens: 384   # Reduced from 512
  max_chunk_tokens: 384       # Reduced from 512
  validation_split: 0.15

# Paths
paths:
  data_dir: "./data"
  train_csv: "train - train.csv"
  test_csv: "test - test.csv"
  novel_files:
    "In Search of the Castaways": "In search of the castaways.txt"
    "The Count of Monte Cristo": "The Count of Monte Cristo.txt"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"

# Training - AUDIT FIXED
training:
  epochs: 50            # Reduced from 100 (early stopping will trigger first)
  learning_rate: 0.00005  # Reduced 6x from 0.0003 (smoother convergence)
  weight_decay: 0.1     # Increased from 0.05 (more regularization)
  warmup_epochs: 3      # Reduced from 5
  gradient_accumulation: 1
  max_grad_norm: 0.5    # Reduced from 1.0 (tighter gradient clipping)
  use_amp: true
  compile: false
  log_freq: 5
  save_every: 5         # More frequent saves
  early_stopping_patience: 5  # NEW: stop if no improvement for 5 epochs

# Loss
loss:
  margin_weight: 0.1
  margin: 0.3

# Reproducibility
seed: 1337

# Device
device: cuda
dtype: bfloat16
         # Native H100 precision
