# MAXIMUM H100 Configuration
# Uses full 81GB VRAM for maximum throughput

# Model Architecture - LARGER for H100
model:
  n_layer: 12           # Double the layers
  n_embd: 512           # Double embedding dimension
  n_head: 8             # More attention heads
  mlp_multiplier: 256   # Larger MLP
  vocab_size: 256
  dropout: 0.15

# Retrieval - MAXIMUM chunks (uses more memory)
retrieval:
  top_k: 100            # Process 100 chunks (uses ~60GB per GPU)
  similarity_type: cosine

# Aggregation
aggregation:
  strategy: max
  top_k: 10

# Data Processing - Larger sequences
data:
  chunk_size: 2048      # Larger chunks
  chunk_overlap: 512
  max_backstory_tokens: 1024   # Double token length
  max_chunk_tokens: 1024       # Double token length
  validation_split: 0.15

# Paths
paths:
  data_dir: "./data"
  train_csv: "train - train.csv"
  test_csv: "test - test.csv"
  novel_files:
    "In Search of the Castaways": "In search of the castaways.txt"
    "The Count of Monte Cristo": "The Count of Monte Cristo.txt"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"

# Training - AGGRESSIVE for dual H100
training:
  epochs: 100
  learning_rate: 0.0005    # Higher LR for larger model
  weight_decay: 0.05
  warmup_epochs: 3
  gradient_accumulation: 1 # No accumulation - use full batch
  max_grad_norm: 1.0
  use_amp: true
  compile: false           # Disabled due to python3-dev
  log_freq: 5
  save_every: 10

# Loss
loss:
  margin_weight: 0.1
  margin: 0.3

# Reproducibility
seed: 1337

# Device
device: cuda
dtype: bfloat16
